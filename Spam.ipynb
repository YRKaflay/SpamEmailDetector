{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc6e329",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ced0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b406ba",
   "metadata": {},
   "source": [
    "## Model and DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfe5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        # Initialize the dataset with texts, labels, tokenizer, and maximum sequence length\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text and label for the given index\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "    # Tokenize the text using the provided tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Return a dictionary of input IDs, attention masks, and labels\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcbc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs=5,\n",
    "    patience=2,\n",
    "):\n",
    "    # Initialize the best F1 score and patience counter\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "\n",
    "    # Loop over each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        # Iterate over batches in the training loader\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            # Move input data and labels to the specified device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass: compute model outputs and loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()  # Backward pass: compute gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()  # Update model parameters\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Iterate over batches in the validation loader\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"]\n",
    "\n",
    "                # Forward pass: compute model outputs\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # Get predictions\n",
    "\n",
    "                val_preds.extend(preds)  # Collect predictions\n",
    "                val_labels.extend(labels.numpy())  # Collect true labels\n",
    "        \n",
    "        # Calculate F1 score for the current epoch\n",
    "        current_f1 = f1_score(val_labels, val_preds)\n",
    "        print(f\"Epoch {epoch+1} - Validation F1: {current_f1:.4f}\")\n",
    "\n",
    "        # Check if the current F1 score is the best so far\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_model = copy.deepcopy(model)  # Save the best model\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter\n",
    "\n",
    "        # Early stopping if no improvement for a number of epochs equal to patience\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return best_model, best_f1  # Return the best model and its F1 score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89fb54",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b6a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0f9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_df = pd.read_csv(r\"C:\\Users\\USER\\Projects\\NLP\\Spam\\data\\SMS_train.csv\", encoding='iso-8859-1')\n",
    "test_df = pd.read_csv(r\"C:\\Users\\USER\\Projects\\NLP\\Spam\\data\\SMS_test.csv\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea2388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.                                       Message_body     Label\n",
       "0       1                         Rofl. Its true to its name  Non-Spam\n",
       "1       2  The guy did some bitching but I acted like i'd...  Non-Spam\n",
       "2       3  Pity, * was in mood for that. So...any other s...  Non-Spam\n",
       "3       4               Will ü b going to esplanade fr home?  Non-Spam\n",
       "4       5  This is the 2nd time we have tried 2 contact u...      Spam"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836fc2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UpgrdCentre Orange customer, you may now claim...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>URGENT! Your Mobile number has been awarded wi...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Someone has contacted our dating service and e...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.                                       Message_body Label\n",
       "0       1  UpgrdCentre Orange customer, you may now claim...  Spam\n",
       "1       2  Loan for any purpose £500 - £75,000. Homeowner...  Spam\n",
       "2       3  Congrats! Nokia 3650 video camera phone is you...  Spam\n",
       "3       4  URGENT! Your Mobile number has been awarded wi...  Spam\n",
       "4       5  Someone has contacted our dating service and e...  Spam"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12243095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to numerical values\n",
    "label_map = {\"Non-Spam\": 0, \"Spam\": 1}\n",
    "train_df[\"label\"] = train_df[\"Label\"].map(label_map)\n",
    "test_df[\"label\"] = test_df[\"Label\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef7fde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Non-Spam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>Non-Spam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Non-Spam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Non-Spam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.                                       Message_body     Label  label\n",
       "0       1                         Rofl. Its true to its name  Non-Spam      0\n",
       "1       2  The guy did some bitching but I acted like i'd...  Non-Spam      0\n",
       "2       3  Pity, * was in mood for that. So...any other s...  Non-Spam      0\n",
       "3       4               Will ü b going to esplanade fr home?  Non-Spam      0\n",
       "4       5  This is the 2nd time we have tried 2 contact u...      Spam      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446c826",
   "metadata": {},
   "source": [
    "## Tokenizer & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3ad4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f484bfa2eb40869c5b21493d614025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c62843551c4f9ba18cba2d8d04dce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97924821d914e1183e279f368ac90ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e53a46ee715491288baaf2c3a9ba963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer and device\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f931c",
   "metadata": {},
   "source": [
    "## Model Initialization & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513db549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349a3e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c9b77bdf9947f19102672866b75ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 48/48 [00:20<00:00,  2.40it/s, loss=0.0090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:20<00:00,  2.29it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 48/48 [00:20<00:00,  2.34it/s, loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 0.9388\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 48/48 [00:21<00:00,  2.24it/s, loss=0.0682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:22<00:00,  2.11it/s, loss=0.0057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 48/48 [00:33<00:00,  1.44it/s, loss=0.0016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:21<00:00,  2.28it/s, loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation F1: 1.0000\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 48/48 [00:23<00:00,  2.06it/s, loss=0.0109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:23<00:00,  2.07it/s, loss=0.0025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 48/48 [00:29<00:00,  1.61it/s, loss=0.0016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 0.9583\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 48/48 [00:31<00:00,  1.52it/s, loss=0.0089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:31<00:00,  1.54it/s, loss=0.0026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 48/48 [00:34<00:00,  1.39it/s, loss=0.0017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:22<00:00,  2.11it/s, loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation F1: 0.9583\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 48/48 [00:25<00:00,  1.88it/s, loss=0.0608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:30<00:00,  1.55it/s, loss=0.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:38<00:00,  1.25it/s, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation F1: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 48/48 [00:24<00:00,  1.98it/s, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation F1: 0.9583\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    skf.split(train_df[\"Message_body\"], train_df[\"label\"])\n",
    "):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "\n",
    "    # Split the data into training and validation sets based on current fold indices\n",
    "    train_texts = train_df[\"Message_body\"].iloc[train_idx].values\n",
    "    train_labels = train_df[\"label\"].iloc[train_idx].values\n",
    "    val_texts = train_df[\"Message_body\"].iloc[val_idx].values\n",
    "    val_labels = train_df[\"label\"].iloc[val_idx].values\n",
    "\n",
    "    # Create dataset objects for training and validation\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Create DataLoader objects for training and validation\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # Initialize the BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    )\n",
    "    model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    # Initialize optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.01, total_iters=len(train_loader) * 5\n",
    "    )\n",
    "\n",
    "    # Train the model and obtain the best model and its F1 score\n",
    "    best_model, fold_f1 = train_model(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, device\n",
    "    )\n",
    "    f1_scores.append(fold_f1)  # Record the F1 score for the current fold\n",
    "\n",
    "    # Clean up to free memory\n",
    "    del model, best_model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c3bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation F1 Scores: [0.9387755102040817, 1.0, 0.9583333333333334, 0.9583333333333334, 0.9583333333333334]\n",
      "Average F1 Score: 0.9628\n"
     ]
    }
   ],
   "source": [
    "# Print cross-validation results\n",
    "print(f\"\\nCross-validation F1 Scores: {f1_scores}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc84d2",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2422e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on the full training dataset\n",
    "final_dataset = TextDataset(\n",
    "    train_df[\"Message_body\"].values, train_df[\"label\"].values, tokenizer\n",
    ")\n",
    "final_loader = DataLoader(final_dataset, batch_size=16, shuffle=True)\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"Message_body\"].values, test_df[\"label\"].values, tokenizer\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07bb2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the final BERT model for sequence classification\n",
    "final_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "final_model.to(device)  # Move the final model to the specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e19e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize optimizer and learning rate scheduler for the final model\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=1.0, end_factor=0.01, total_iters=len(final_loader) * 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667bffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the final dataset into training and validation subsets for additional training\n",
    "train_size = int(0.9 * len(final_dataset))\n",
    "val_size = len(final_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    final_dataset, [train_size, val_size]\n",
    ")\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309ad91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 54/54 [00:24<00:00,  2.23it/s, loss=0.0304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation F1: 0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 54/54 [00:22<00:00,  2.37it/s, loss=0.0030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation F1: 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 54/54 [00:28<00:00,  1.88it/s, loss=0.0017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation F1: 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 54/54 [00:24<00:00,  2.21it/s, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation F1: 0.9524\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Train the final model using the entire training data\n",
    "final_model, _ = train_model(\n",
    "    final_model, train_loader, val_loader, optimizer, scheduler, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9515",
   "metadata": {},
   "source": [
    "## Final Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56ed9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "final_model.eval()  # Set the model to evaluation mode\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Forward pass: compute model outputs\n",
    "        outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # Get predicted labels\n",
    "        test_preds.extend(preds)  # Collect predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ad28f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a submission DataFrame with predicted labels\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"S. No.\": test_df[\"S. No.\"],\n",
    "        \"Label\": [\"Spam\" if pred == 1 else \"Non-Spam\" for pred in test_preds],\n",
    "    }\n",
    ")\n",
    "# Save predictions to CSV\n",
    "submission.to_csv(r\"C:\\Users\\USER\\Projects\\NLP\\Spam\\data\\submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
